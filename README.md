# Smart-Review-AI: Fine-Tuning GPT-2 LLM with LoRA

Generate realistic, human-like product reviews using a fine-tuned GPT-2 LLM model enhanced with LoRA.

## ğŸš€ Project Overview

Smart Review AI is a lightweight, AI-powered application designed to generate human-like product reviews. It leverages a GPT-2 language model fine-tuned with LoRA (Low-Rank Adaptation) to produce coherent, context-aware, and natural-sounding reviews for any product.

This app demonstrates how LoRA can efficiently adapt large language models without the need to retrain the entire model, making it faster, cheaper, and easier to deploy.

## âœ¨ Features

- Humanized Review Generation â€“ Generates realistic reviews with controllable tone and ratings.
- Fine-Tuning Flexibility â€“ Easily fine-tune GPT-2 for new product categories or domains.
- Efficient & Lightweight â€“ Uses LoRA adapters, keeping the model small and fast.
- Streamlit Interface â€“ Intuitive web app for generating reviews and evaluating outputs.
- Hugging Face Spaces Deployment â€“ Live demo available in the cloud.

## ğŸ› ï¸ Tech Stack

- Language Model: GPT-2
- Fine-Tuning: LoRA (Low-Rank Adaptation) via PEFT
- Web App: Streamlit
- Deployment: Hugging Face Spaces
- Python Libraries: transformers, peft, torch, numpy, pandas

## ğŸ’» How It Works

- Select a Base Model â€“ GPT-2 is loaded as the base LLM.
- Apply LoRA Adapter â€“ Fine-tuned LoRA weights are loaded to adapt the model to product reviews.
- Start Fine-tuning with your dataset
- Generate Review â€“ Enter product details, category, features, rating, and tone. The model generates a human-like review.
- Reviews Evaluation â€“ Track average words length, Tune match ratio, and perplexity.

ğŸ–¥ï¸ Live Demo

Check out the web app on Hugging Face Spaces:

https://huggingface.co/spaces/shibbir24/SmartReviewAI

## âš¡Usage Example
```
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

### Load base model and LoRA adapter
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")
model = PeftModel.from_pretrained(model, "lora_adapter")
model.eval()

### Generate review
inputs = tokenizer("Product: Amazon Kindle\nCategory: E-Reader\nFeatures: Lightweight, long battery life\nRating: 5 stars\nTone: Enthusiastic", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=150)
print(tokenizer.decode(outputs[0]))
```

## ğŸ“‚ Repository Structure
```
Smart-Review-AI/
â”‚
â”œâ”€â”€ app.py               # Streamlit web app
â”œâ”€â”€ lora_adapter/        # LoRA adapter files (.safetensors + config)
â”œâ”€â”€ requirements.txt     # Required Python packages
â”œâ”€â”€ evaluate_model.py    # Model reviews evaluation
â”œâ”€â”€ finetune_lora.py     # Model fine-tuning
â””â”€â”€ README.md            # Project documentation
```

## ğŸ’¡ Why LoRA?

- Fine-tuning large language models traditionally requires gigabytes of storage and GPU resources.
- LoRA allows parameter-efficient fine-tuning, updating only a small subset of weights.
- Faster training, smaller model files, and seamless integration with GPT-2.

## ğŸ“ License

This project is licensed under the MIT License â€“ feel free to use, modify, and deploy it.

## ğŸ™Œ Contribution

Contributions, ideas, or improvements are welcome. Open an issue or submit a pull request to collaborate.

